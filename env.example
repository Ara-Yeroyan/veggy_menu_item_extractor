# LLM Configuration
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2

# OpenAI fallback (if LLM_PROVIDER=openai)
OPENAI_API_KEY=your-api-key-here
OPENAI_MODEL=gpt-4o-mini

# MCP Server
MCP_SERVER_URL=http://mcp-server:8001

# Langsmith Observability
LANGCHAIN_TRACING_V2=true
LANGCHAIN_PROJECT=menu-parsing
LANGCHAIN_API_KEY=your-langsmith-api-key

# Classification Thresholds
CONFIDENCE_THRESHOLD=0.6
HITL_THRESHOLD=0.4

# RAG Configuration
RAG_TOP_K=5
EMBEDDING_MODEL=all-MiniLM-L6-v2

# LLM Batch Processing (reduces latency)
LLM_BATCH_ENABLED=true
LLM_BATCH_SIZE=8

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
MCP_HOST=0.0.0.0
MCP_PORT=8001
